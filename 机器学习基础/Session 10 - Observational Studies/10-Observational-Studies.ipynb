{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Abstract and Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this document is to briefly introduce the general approaches for causal inference **without** randomized experiments, i.e., the observational studies. In this document, we will first revisit the notion of selection bias and to introduce the central notion of identification in observational studies. We discuss the idea of quasi-randomization and provide the general frameworks of regression and matching for observational studies.\n",
    "\n",
    "Below is the outline of this document:\n",
    "\n",
    "- In [Section 1](#section_1), we discuss selection bias again and introduce the notion of identification in observational studies.\n",
    "\n",
    "- In [Section 2](#section_2), we introduce the mathematical model for causal inference in observational studies. Meanwhile, we discuss the key Conditional Independence Assumption (CIA) for valid causal inference without experiment.\n",
    "\n",
    "- In [Section 3](#section_3), we introduce regression analysis for observational studies. \n",
    "\n",
    "- In [Section 4](#section_4), we introduce the matching analysis for observational studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_1'></a>\n",
    "# 1. Identification without Complete Randomization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "In causal inference, identification refers to **accurately estimating the effect of a treatment when the sample size of the data goes to infinity**. \n",
    "\n",
    "</font>\n",
    "    \n",
    "-----------------\n",
    "\n",
    "A key insight from the potential outcome model is that, in the presence of **selection bias**, identification is **impossible**. The key to remove selection bias is to have sufficiently random treatment assignment mechanism $\\mathcal W:i\\rightarrow W_i$, which we usually refer to as the **identification strategy** (i.e., the treatment assignment mechanism to identify the causal effect of the treatment).\n",
    "\n",
    "- The **ideal** identification strategy is fully randomized treatment assignment by the researcher/analyst. This is also called randomized experiment/AB testing/randomized controlled trials in different contexts. Two kinds of experiments are widely adopted in practice and in research:\n",
    " - **<font color=\"red\">Lab Experiment.</font>** Lab experiments are the cornerstone of modern natural sciences and behaviral economics. A lab experiment is conducted in a lab environment fully controlled by the researcher so that the experimental results satisfy full **internal validity**, but may face some concerns with **external validity**. An example of lab experiment: [Mendel's experiments on inheritance](https://www.nature.com/scitable/topicpage/gregor-mendel-and-the-principles-of-inheritance-593/).\n",
    " - **<font color=\"red\">Field Experiment.</font>** Field experiments are very popular for social and economic studies, and for tech firms as well (called A/B testing in tech firms). A field experiment is conducted in the real field where the treatment is relevant in practice. By definition, field experiments better satisfy **external validity** than lab experiments, but they are also **much more expensive** (in terms of monetary and social capital costs). An example of field experiment: [Oregan health insurance experiment](https://www.nber.org/programs-projects/projects-and-centers/oregon-health-insurance-experiment?page=1&perPage=50).\n",
    " \n",
    " Because randomized experiments completely remove selection bias, they are considered as the **<font color=\"red\">gold standard</font>** for causal inference.\n",
    "\n",
    "- The less ideal but still **reasonable** identification strategy is haphazard treatment assignment by something outside the control of the researcher. This is also called the **<font color=\"red\">natural experiment</font>** or **<font color=\"red\">quasi-experiment</font>**. \n",
    " - For a natural experiment, the treatment assignment mechanism is driven by weather, birthdays, child gender, arbitrary rules. We will discuss causal inference methods based on these quasi-experiments later in this course.\n",
    " \n",
    "- The bottom-line identification strategy is that the treatment assignment is **\"as-if\" random after statistical control** (i.e., regression, matching, and re-weighting of data, etc.). Frankly speaking, such an identification strategy is generally not clean (i.e., producing a biased estimation of causal effect). However, sometimes this is the only thing you can do.  \n",
    "\n",
    "- Finally, if the treatment is self-selected and no plausible control is available, the causal effect is **not identifiable**.\n",
    "\n",
    "Although full randomization is the gold standard for causal inference, it is infeasible for most of the problems in practice. For example, we are interested in evaluating the causal effect of education length on a worker's future earning. It is impossible (not ethical and too costly) to conduct a randomized experiment in which part of the randomly selected workers receive more education than other workers. Therefore, observational studies are unavoidable for the causal inference in a great variety of problems.\n",
    "\n",
    "The key idea is to design observational studies to **approximate randomized experiments** by **adjusting the observable features** with the hope that unobserved features are **well-balanced** as well. In a nutshell, the planner of an observational study should always ask himself: \n",
    "\n",
    "- How would the study be conducted if it were possible to do it by randomzied controlled experimentation?\n",
    "\n",
    "Let us now compare (i) randomized experiments, (ii) good observational studies, and (iii) bad observational studies.\n",
    "\n",
    "- **Randomized Experiments:** \n",
    " - Treatment is well-defined, well-controlled, and randomly assigned.\n",
    " - There should be a clear distinction between features and outcomes. Importantly, **<font color=\"red\">the features should be measured before the treatment starts, whereas the outcome should be measured after the treatment starts</font>**.\n",
    " - Example: A/B testing of Didi's coupon.\n",
    " \n",
    "- **Good Observational Study:**\n",
    " - Treatment is well-defined and the researcher should have a precise knowledge of treatment assignment mechanism. At the minimal, the researcher should be able to convincingly answer the following question: **<font color=\"red\">Why do two units who are identical on measured covariates receive different treatments?</font>** Although the treatment assignment may not be fully random, the circumstances for the study should be chosen so that the treatment seems haphazard, or at least not obviously related to potential outcomes.  \n",
    " - There should be a clear distinction between features and outcomes. Importantly, **<font color=\"red\">the features should be measured before the treatment starts, whereas the outcome should be measured after the treatment starts</font>**.\n",
    "\n",
    "- **Bad Observational Study:**\n",
    " - No attention is given to the treatment assignment process so that the researcher does not even know when the treatment began or what the treatment really is. The knowledge of assignment mechanism is not precise for the researcher as well. The units may self-select into the treatment/control group based on their potential outcomes. \n",
    " - The distinction between features and outcomes is blurred. It may occur that **<font color=\"red\">the outcome variable could reversely impact some of the features.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Examples of Good and Bad Observational Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now give an example of good observational study and an example of bad observational study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of Good Observational Study: Regression Discontinuity Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first introduce an example of good observational study. The goal is to evaluating the impact of class size on the academic performance of the students. This is a very challenging causal inference task, because it is impossible to randomly assign students to big and small classes. In practice, elite schools usually have smaller class sizes, so the assignment of treatment condition (i.e., small classes) is not random. \n",
    "\n",
    "A clever identification strategy is called **[Regression Discontinuity Design (RDD)](https://en.wikipedia.org/wiki/Regression_discontinuity_design)**. Specifically, the public school system of the USA follows the so called [Maimonides' rule](https://en.wikipedia.org/wiki/Maimonides%27_rule): (i) if there are fewer than or equal to 25 students in a class, it will have one teacher, (ii) if there are more than 25 students but fewer than or equal to 40 students in a class, it will have one teacher and one TA, (iii) if there are more than 40 students in a class, it should be split into 2, each having 1 teacher. The idea is to examine a group of classes with 31 to 40 students (the control group) and another group of classes with 41 to 50 students (the treatment group). Because when the total number of students is close to 40, the exact number is \"close to\" random, so that the assignment of the sample units into the treatment or control group is \"somewhat\" random. Therefore, comparing the average and verbal scores of the treatment and control groups (treatment classes have 5+ more points than control classes) demonstrate that smaller class sizes do improve the pedagogical effectiveness. You may also refer to [this paper](http://piketty.pse.ens.fr/fichiers/AngristLavy1999.pdf) for more details of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of Bad Observational Study: Simpson's Paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) refers to a very general phenomenon in causal inference that a trend appears in several different groups of data but disappears or reverses when these groups are combined. Let us now discuss a typical example of Simpson's Paradox: UC Berkeley's gender bias case. We remark that the data below is faked and for pedagogical purposes only.\n",
    "\n",
    "It has been observed that girl’s admission rate is lower than the boy’s at UC Berkeley, so people are suspecious that the University has some gender bias in its admission process. Consider the following table articulating the admission data. \n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th> </th>\n",
    "    <th>Business</th> \n",
    "    <th>Engineering</th>\n",
    "    <th>Total</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Boys Accepted</td>\n",
    "    <td>20</td>\n",
    "    <td>200</td>\n",
    "    <td>220</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Girls Accepted</td>\n",
    "    <td>50</td>\n",
    "    <td>50</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Boys Rejected</td>\n",
    "    <td>50</td>\n",
    "    <td>250</td>\n",
    "    <td>300</td>  \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Girls Rejected</td>\n",
    "    <td>100</td>\n",
    "    <td>50</td>\n",
    "    <td>150</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Boys' Acceptance Rate</td>\n",
    "    <td>2/7</td>\n",
    "    <td>4/9</td>\n",
    "    <td>11/26</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Girls' Acceptance Rate</td>\n",
    "    <td>1/3</td>\n",
    "    <td>1/2</td>\n",
    "    <td>2/5</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "----------\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "**Puzzle:**\n",
    "\n",
    "- Business school: Girl's acceptance rate (1/3) > Boy's acceptance rate (2/7)\n",
    "- Engineering school: Girl's acceptance rate (1/2) > Boy's acceptance rate (4/9)\n",
    "- Overall: Girl's acceptance rate (2/5) < Boy's acceptance rate (11/26)\n",
    "\n",
    "</font>\n",
    "    \n",
    "----------\n",
    "\n",
    "As we can observe from the table above, the acceptance rate for girls into business (2/7) is higher than that for boys (1/3), similar is true for girls accepted into engineering (1/2) higher than boys accepted into engineering (4/9). However, the total acceptance rate for girls (2/5) is lower than that for boys (11/20). This puzzling phenomenon is driven by the rationale that the acceptance rate into engineering school (5/11) is much higher than that into business school (7/22). The porportion of boys applying to engineering school (45/51) is also much higher than that of girls applying to engineering school (2/5), making the overall acceptance rate for boys higher than that of girls.\n",
    "\n",
    "The Simpson's Paradox is essentially driven by an **<font color=\"red\">omitted variable bias</font>**. In the example above, the omitted variable is which school a student is applying to (business or engineering?). Simpson's Paradox and other correlations that are not driven by causality is usually referred to as [spurious correlations](https://en.wikipedia.org/wiki/Spurious_relationship)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_2'></a>\n",
    "# 2. Model for Observational Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider $n$ subjects $\\{1,2,...,n\\}$ where $n$ is the sample size. We use $W_i\\in\\{0,1\\}$ to represent whether subject $i$ is assigned into the treatment group. The assignment mechanism $\\mathcal W$ is not necessarily random. Define $n_1=\\sum_{i=1}^nW_i$ as the number of treated subjects, and $n_0=n-n_1$ as the number of untreated subjects. The potential outcomes of subject $i$ with respect to different treatment assignments are given by $Y_i(1)$ (treated) and $Y_i(0)$ (untreated).\n",
    "\n",
    "We define the **<font color=\"red\">pre-treatment</font>** features for subject $i$ as $X_i=(X_{i1},X_{i2},...,X_{ip})\\in\\mathbb R^p$, which is pre-determined and causally precedent with respect to the treatment $W_i$. Typical features include demographic information such as age, gender, and location. In general, the features are context-specific. Furthermore, because the treatment assignment is not completely random, $X_i$ may be correlated with both $W_i$ and $Y_i$, thus **<font color=\"red\">confounding</font>** the causal inference. In general, we should exclude any possible correlations (between features and outcomes) that are potentially affected by $W_i$ (i.e., post-treatment features should be excluded from $X_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of central importance for causal inference is the so-called **<font color=\"red\">Conditional Independence/Ignorability assumption (CIA)</font>** which dictates that, conditioned on each possible feature, the assignment of treatment is randomized. Mathmatically, we write:\n",
    "<font color=\"red\">\n",
    "$$\\{Y_i(1),Y_i(0)\\}\\perp W_i|X_i=x,\\mbox{ for all feature }x,$$\n",
    "</font>\n",
    "where $\\perp$ refers to uncorrelated (with zero correlation). Under CIA, although treatment is not randomly assigned, it is \"almost random\" if conditioned on features. In particular, in a randomized experiment, $\\{Y_i(1),Y_i(0)\\}\\perp W_i$, which is stronger than CIA. In addition, we also make the **<font color=\"red\">common support assumption</font>** for treatment and control conditions:\n",
    "<font color=\"red\">\n",
    "$$0<\\mathbb P[W_i=1|X_i=x]<1\\mbox{ for all }x,$$\n",
    "</font>\n",
    "i.e., for any feature $X_i=x$, there is a positive probability of assigning the subject to both the treatment group and the control group.\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"font-family:Comic Sans MS\">\n",
    "    <p style=\"color:red\">\n",
    "If the CIA and the common support assumption hold, $ATE$ is identifiable.\n",
    "        </p>\n",
    "</span>    \n",
    "\n",
    "----\n",
    "\n",
    "Causal inference in observational studies relies critically on CIA. We will discuss a couple of identification strategies under this assumption.  The intuition is to find the strata (i.e., groups) of features $X$ in which the assignment $W_i$ is \"as-if\" random. Our goal is to approximate a randomized experiment in each stratum of subjects. To verify CIA, it suffices to argue that the variation in the treatment assignment within each stratum of $X$ is random.\n",
    "\n",
    "In this document, we will introduce two commonly used approaches:\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "- **Regression**;\n",
    "- **Matching**.\n",
    "\n",
    "</font>    \n",
    "    \n",
    "For either approach, the biggest issue is that we cannot balance the **<font color=\"red\">unobservables</font>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_3'></a>\n",
    "# 3. Observational Studies with Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the regression analysis for observational studies is identical to that for causal inference. For completeness, we formalize everything again in this section, and revisit the case of Didi's coupon case. The only new here is that we do not assume $W_i\\in\\{0,1\\}$ is fully random, so that this is an observational study setting. \n",
    "\n",
    "Consider the case where the platform has a higher likelihood to giving a coupon to a user with a lower activeness and/or spending in the previous week. Therefore, the treatment is no longer fully randomized. We make the assumption that CIA holds, i.e., given the features, the treatment is randomly assigned for the user.\n",
    "\n",
    "To begin our analysis, we first load the necessary packages and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_active</th>\n",
       "      <th>new_spending</th>\n",
       "      <th>Coupon</th>\n",
       "      <th>Male</th>\n",
       "      <th>ActiveDays</th>\n",
       "      <th>Spending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.240780</td>\n",
       "      <td>5.250043</td>\n",
       "      <td>0.397410</td>\n",
       "      <td>0.503680</td>\n",
       "      <td>1.404960</td>\n",
       "      <td>28.118527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.427559</td>\n",
       "      <td>10.226197</td>\n",
       "      <td>0.489365</td>\n",
       "      <td>0.499989</td>\n",
       "      <td>1.055452</td>\n",
       "      <td>23.641811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.590000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>197.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           is_active   new_spending         Coupon           Male  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.240780       5.250043       0.397410       0.503680   \n",
       "std         0.427559      10.226197       0.489365       0.499989   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       1.000000   \n",
       "75%         0.000000       0.000000       1.000000       1.000000   \n",
       "max         1.000000      56.590000       1.000000       1.000000   \n",
       "\n",
       "          ActiveDays       Spending  \n",
       "count  100000.000000  100000.000000  \n",
       "mean        1.404960      28.118527  \n",
       "std         1.055452      23.641811  \n",
       "min         0.000000       0.000000  \n",
       "25%         1.000000      12.600000  \n",
       "50%         1.000000      23.580000  \n",
       "75%         2.000000      41.840000  \n",
       "max         6.000000     197.950000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Didi_new  = pd.read_csv(\"Didi_new.csv\")\n",
    "df_Didi_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has 5 variables:\n",
    "\n",
    "- **is_active $\\in\\{0,1\\}$:** Whether the user is active on the day of the experiment, 1=active, and 0=inactive;\n",
    "- **new_spending $\\in\\mathbb R^+$:** The amount of spending by user $i$ on the day of the experiment;\n",
    "- **Coupon $\\in\\{0,1\\}$:** Whether user $i$ is in the treatment group (i.e., $W_i$ in the potential outcome model), 1=treatment, and 0=control, **not fully random**;\n",
    "- **Male $\\in\\{0,1\\}$:** 1 means male 0 means female;\n",
    "- **ActiveDays $\\in\\mathbb Z^+$:** The number of actives for user $i$ 1 week before the experiment (i.e., days $t-7,t-6,t-5,...,t-1$);\n",
    "- **Spending $\\in\\mathbb R^+$:** The amount of spending by user $i$ 1 week before the experiment (i.e., days $t-7,t-6,t-5,...,t-1$).\n",
    "\n",
    "We now conduct balance checks to verify that the treatment assignment is not fully random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   Male   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.3496\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):              0.554\n",
      "Time:                        14:00:26   Log-Likelihood:                -72576.\n",
      "No. Observations:              100000   AIC:                         1.452e+05\n",
      "Df Residuals:                   99998   BIC:                         1.452e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5044      0.002    247.661      0.000       0.500       0.508\n",
      "Coupon        -0.0019      0.003     -0.591      0.554      -0.008       0.004\n",
      "==============================================================================\n",
      "Omnibus:                   340880.653   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16666.434\n",
      "Skew:                          -0.015   Prob(JB):                         0.00\n",
      "Kurtosis:                       1.000   Cond. No.                         2.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Balance check for feature Male.\n",
    "\n",
    "# OLS specification: Male ~ beta_0 + beta_1 * coupon + epsion\n",
    "\n",
    "BalanceCheck_Male = sm.OLS(endog = df_Didi_new['Male'], exog = sm.add_constant(df_Didi_new['Coupon']))\n",
    "result_BC_Male = BalanceCheck_Male.fit()\n",
    "print(result_BC_Male.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the feature **Male** is balanced with respect to treatment and control groups. Next, we conduct balance checks for the **ActiveDays** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             ActiveDays   R-squared:                       0.220\n",
      "Model:                            OLS   Adj. R-squared:                  0.220\n",
      "Method:                 Least Squares   F-statistic:                 2.828e+04\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):               0.00\n",
      "Time:                        14:00:26   Log-Likelihood:            -1.3484e+05\n",
      "No. Observations:              100000   AIC:                         2.697e+05\n",
      "Df Residuals:                   99998   BIC:                         2.697e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.8074      0.004    476.114      0.000       1.800       1.815\n",
      "Coupon        -1.0127      0.006   -168.171      0.000      -1.024      -1.001\n",
      "==============================================================================\n",
      "Omnibus:                     6939.469   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8936.749\n",
      "Skew:                           0.640   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.711   Cond. No.                         2.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Balance check for feature ActiveDays.\n",
    "\n",
    "# OLS specification: ActiveDays ~ beta_0 + beta_1 * coupon + epsion\n",
    "\n",
    "\n",
    "BalanceCheck_ActiveDays = sm.OLS(endog = df_Didi_new['ActiveDays'], exog = sm.add_constant(df_Didi_new['Coupon']))\n",
    "result_BC_ActiveDays = BalanceCheck_ActiveDays.fit()\n",
    "print(result_BC_ActiveDays.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our balance check result above implies that the average number of active days for users in the treatment group (receiving a coupon) is significantly lower than that for users in the control group (not receiving a coupon) by a magnitude of 1.0127 days. This implies that, the treatment and control group samples are **<font color=\"red\">not balanced</font>**.\n",
    "\n",
    "Finally, we conduct balance checks for the variable **Spending**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Spending   R-squared:                       0.218\n",
      "Model:                            OLS   Adj. R-squared:                  0.218\n",
      "Method:                 Least Squares   F-statistic:                 2.793e+04\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):               0.00\n",
      "Time:                        14:00:26   Log-Likelihood:            -4.4588e+05\n",
      "No. Observations:              100000   AIC:                         8.918e+05\n",
      "Df Residuals:                   99998   BIC:                         8.918e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         37.0893      0.085    435.574      0.000      36.922      37.256\n",
      "Coupon       -22.5732      0.135   -167.119      0.000     -22.838     -22.308\n",
      "==============================================================================\n",
      "Omnibus:                    19800.980   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            47627.898\n",
      "Skew:                           1.111   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.549   Cond. No.                         2.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Balance check for feature Spending.\n",
    "\n",
    "# OLS specification: Spending ~ beta_0 + beta_1 * coupon + epsion\n",
    "\n",
    "BalanceCheck_Spending = sm.OLS(endog = df_Didi_new['Spending'], exog = sm.add_constant(df_Didi_new['Coupon']))\n",
    "result_BC_Spending = BalanceCheck_Spending.fit()\n",
    "print(result_BC_Spending.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above balance check result implies that the average spending of the previous week prior to the treatment for users in the treatment group (receiving a coupon) is significantly lower than that for users in the control group (not receiving a coupon) by a magnitude of RMB -22.70. This, again, suggests that, the treatment and control group samples are **not balanced**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the F-test to check the balancedness of the treatment and control groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Coupon   R-squared:                       0.232\n",
      "Model:                            OLS   Adj. R-squared:                  0.232\n",
      "Method:                 Least Squares   F-statistic:                 1.004e+04\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):               0.00\n",
      "Time:                        14:02:13   Log-Likelihood:                -57260.\n",
      "No. Observations:              100000   AIC:                         1.145e+05\n",
      "Df Residuals:                   99996   BIC:                         1.146e+05\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.7031      0.003    266.668      0.000       0.698       0.708\n",
      "Male          -0.0002      0.003     -0.064      0.949      -0.005       0.005\n",
      "ActiveDays    -0.1197      0.003    -41.486      0.000      -0.125      -0.114\n",
      "Spending      -0.0049      0.000    -37.965      0.000      -0.005      -0.005\n",
      "==============================================================================\n",
      "Omnibus:                    33921.797   Durbin-Watson:                   2.007\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5681.892\n",
      "Skew:                           0.241   Prob(JB):                         0.00\n",
      "Kurtosis:                       1.936   Cond. No.                         92.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Balance check based on F-statistic.\n",
    "\n",
    "# OLS specification: Coupon ~ alpha_0 + alpha_1 * Male + alpha_2 * ActiveDays + alpha_3 * Spending + epsion\n",
    "\n",
    "BalanceCheck_all = sm.OLS(endog = df_Didi_new['Coupon'], exog = sm.add_constant(df_Didi_new[['Male','ActiveDays','Spending']]))\n",
    "result_BC_all = BalanceCheck_all.fit()\n",
    "print(result_BC_all.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-value for the F-test is 0.00, suggesting that we should reject the null hypothesis and the treatment and control groups are **imbalanced**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the selection bias verified by our balance checks, estimate the average treatment effect of receiving a coupon using the following OLS linear regression will yield a biased result:\n",
    "\n",
    "$$Y_i\\approx \\hat \\beta_0+\\hat\\tau W_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              is_active   R-squared:                       0.003\n",
      "Model:                            OLS   Adj. R-squared:                  0.003\n",
      "Method:                 Least Squares   F-statistic:                     284.2\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):           1.12e-63\n",
      "Time:                        14:03:08   Log-Likelihood:                -56785.\n",
      "No. Observations:              100000   AIC:                         1.136e+05\n",
      "Df Residuals:                   99998   BIC:                         1.136e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.2593      0.002    149.064      0.000       0.256       0.263\n",
      "Coupon        -0.0465      0.003    -16.858      0.000      -0.052      -0.041\n",
      "==============================================================================\n",
      "Omnibus:                    17770.555   Durbin-Watson:                   1.996\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            25449.128\n",
      "Skew:                           1.207   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.473   Cond. No.                         2.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# Fit a simple linear regression model.\n",
    "\n",
    "LinearModel = sm.OLS(endog = df_Didi_new['is_active'], exog = sm.add_constant(df_Didi_new['Coupon']))\n",
    "result = LinearModel.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above analysis, the simple OLS regression analysis gives an estimate indicating that receiving a coupon will \"result in a significant decrease\" in the probability (-0.0465) that the user being active on the day of coupon reception. As discussed above, this biased estimate is due to that less active and fewer spending users are more likely to receive a coupon. In this case, the feature **ActiveDays** and **Spending** are **positively correlated** with both the potential outcomes (**is_active** and **new_spending**) and **negatively correlated** with the treatment assignment (**Coupon**). Therefore, directly running a simple regression of the outcome variable on the treatment assignment will **underestimate** the true causal effect of receiving a coupon. \n",
    "\n",
    "Therefore, we adopt the linear regression model to control the features:\n",
    "\n",
    "$$Y_i\\approx \\hat\\beta_0+\\hat\\tau W_i + \\sum_{j=1}^p\\hat\\beta_jX_{ij}\\mbox{ for all }i=1,2,...,n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              is_active   R-squared:                       0.099\n",
      "Model:                            OLS   Adj. R-squared:                  0.099\n",
      "Method:                 Least Squares   F-statistic:                     2733.\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):               0.00\n",
      "Time:                        14:03:08   Log-Likelihood:                -51739.\n",
      "No. Observations:              100000   AIC:                         1.035e+05\n",
      "Df Residuals:                   99995   BIC:                         1.035e+05\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0033      0.003      0.997      0.319      -0.003       0.010\n",
      "Coupon         0.0965      0.003     32.259      0.000       0.091       0.102\n",
      "Male          -0.0001      0.003     -0.042      0.967      -0.005       0.005\n",
      "ActiveDays     0.1466      0.003     53.239      0.000       0.141       0.152\n",
      "Spending      -0.0002      0.000     -1.941      0.052      -0.000    2.34e-06\n",
      "==============================================================================\n",
      "Omnibus:                    13511.762   Durbin-Watson:                   1.997\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            18775.903\n",
      "Skew:                           1.045   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.629   Cond. No.                         120.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# OLS model with features to examine the impact of receiving coupon on the activeness.\n",
    "\n",
    "# OLS specification: is_active ~ beta_0 + tau * Coupon + beta_1* Male + beta_2 * ActiveDays + beta_3 * Spending + epsion\n",
    "\n",
    "\n",
    "features = ['Coupon','Male','ActiveDays','Spending']\n",
    "\n",
    "OLS_f_active = sm.OLS(endog = df_Didi_new['is_active'], exog = sm.add_constant(df_Didi_new[features]))\n",
    "result_f_new_active = OLS_f_active.fit()\n",
    "print(result_f_new_active.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           new_spending   R-squared:                       0.100\n",
      "Model:                            OLS   Adj. R-squared:                  0.099\n",
      "Method:                 Least Squares   F-statistic:                     2763.\n",
      "Date:                Wed, 23 Feb 2022   Prob (F-statistic):               0.00\n",
      "Time:                        14:03:09   Log-Likelihood:            -3.6915e+05\n",
      "No. Observations:              100000   AIC:                         7.383e+05\n",
      "Df Residuals:                   99995   BIC:                         7.384e+05\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.3657      0.078     -4.687      0.000      -0.519      -0.213\n",
      "Coupon         3.3077      0.072     46.239      0.000       3.167       3.448\n",
      "Male           0.0388      0.061      0.632      0.527      -0.082       0.159\n",
      "ActiveDays    -0.1740      0.066     -2.644      0.008      -0.303      -0.045\n",
      "Spending       0.1610      0.003     54.865      0.000       0.155       0.167\n",
      "==============================================================================\n",
      "Omnibus:                    24412.721   Durbin-Watson:                   1.997\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            48567.582\n",
      "Skew:                           1.495   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.648   Cond. No.                         120.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rphilipzhang/opt/anaconda3/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "# OLS model with features to examine the impact of receiving coupon on the average spending.\n",
    "\n",
    "# OLS specification: new_spending ~ beta_0 + tau * Coupon + beta_1 * Male + beta_2 * ActiveDays + beta_3 * Spending + epsion\n",
    "\n",
    "\n",
    "features = ['Coupon','Male','ActiveDays','Spending']\n",
    "\n",
    "OLS_f_new_spending = sm.OLS(endog = df_Didi_new['new_spending'], exog = sm.add_constant(df_Didi_new[features]))\n",
    "result_f_new_spending = OLS_f_new_spending.fit()\n",
    "print(result_f_new_spending.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By controlling the features, our regression results imply that:\n",
    "- Receiving a Coupon **increases** the probability that a user will be active on the platform by **9.65%**;\n",
    "- Receiving a Coupon **increases** the total spending of a user at the day of reception on the platform by **RMB 3.31**.\n",
    "\n",
    "This is consistent with our business sense and the A/B testing analysis result. Therefore, controlling for the features saves us from the omitted variable bias.\n",
    "\n",
    "As in the analysis of A/B testing, we can also introduce the cross terms of a feature and the treatment assignment variable, which can help estimate the HTE of the treatment with respect to different values of the feature. Students are encouraged to work on their own to estimate the HTE in this observational study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berkson’s Paradox\n",
    "\n",
    "In some situations, controlling for more features may lead to some undesirable outcomes. If 3 quantities of interest $X_1$, $X_2$, and $X_3$ has the following relationship: (i) $X_1$ and $X_3$ are independent of each other, and (ii) they both have some causal effect on $X_2$; as illustrated in the following Figure.\n",
    "\n",
    "<img src=\"Immorality.png\" width=250>\n",
    "\n",
    "Then, if we regress $X_3$ on $X_1$ and $X_2$,\n",
    "\n",
    "$$X_3\\sim X_1+X_2$$\n",
    "\n",
    "we will get a biased estimate of the causal effect of $X_1$ on $X_2$. For example, $X_1$ is the looks of a male and $X_3$ is his kindness. $X_2$ is whether this man is in relationship ($X_2=1$) or available ($X_2=0$). Therefore, good-looking and/or kind men are more likely to be in relationship. If we run the above regression, we will find that the coefficient of $X_1$ is negative. This seems to, mistakenly, suggest that \"good-looking men are jerks\". See the following figure for an illustration.\n",
    "\n",
    "<img src=\"BerksonParadox.png\" width=1200>\n",
    "\n",
    "Therefore, when controlling for features in regression analysis for causal inference, **<font color=\"red\">we cannot include those that are results of the outcome</font>**. The above phenomenon is referred to as the **<font color=\"red\">Berkson’s Paradox</font>** ([see here](https://en.wikipedia.org/wiki/Berkson%27s_paradox) for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section_4'></a>\n",
    "# 4. Observational Studies with Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of matching is essentially to control for the features in a non-parametric fashion. Specifically, we impute the missing potential outcomes using observed outcomes of the \"closest\" $M\\ge1$ units (i.e., the nearest neighbors). Mathematically, we first estimate, for each subject $i$ in the treatment group ($W_i=1$), the potential outcome of this subject if $W_i=0$:\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "$$\\hat Y_i(0)=\\frac{1}{M}\\sum_{m=1}^MY_{j_m(i)}(0),$$\n",
    "    \n",
    "</font>    \n",
    "where $j_m(i)$ is the subject with feature $m^{th}$ closest to subject $i$. With such an estimate of $Y_i(0)$, we can estimate the average treatment effect on treated as:\n",
    "\n",
    "-----\n",
    "\n",
    "<font color = \"red\">\n",
    "$$\\widehat{ATT}=\\frac{1}{n_1}\\sum_{W_i=1}\\left(Y_i(1)-\\hat{Y}_i(0)\\right)=\\frac{1}{n_1}\\sum_{W_i=1}\\left\\{Y_i(1)-\\frac{1}{M}\\sum_{m=1}^MY_{j_m(i)}(0)\\right\\}$$\n",
    "    \n",
    "</font>\n",
    "\n",
    "--------\n",
    "The matching method with $M=1$ can be illustrated as the following figure, where each treated subject is matched with its closest untreated subject.\n",
    "<img src=\"matching1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a matching example in the following table (with $M=1$, so we only consider the nearest neighbor in matching):\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th>Subject </th>\n",
    "    <th>$W_i$</th> \n",
    "    <th>$Y_i(1)$</th>\n",
    "    <th>$Y_i(0)$</th>\n",
    "    <th>$X_i$</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>6</td>\n",
    "    <td>?</td>\n",
    "    <td>3</td>  \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>?</td>\n",
    "    <td>1</td>   \n",
    "    </tr>\n",
    "   <tr>\n",
    "<td>3</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>?</td>\n",
    "    <td>10</td>   \n",
    "     </tr>\n",
    "   <tr>\n",
    "    <td>4</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>2</td>  \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>5</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>9</td>\n",
    "    <td>3</td>  \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>1</td>\n",
    "    <td>-2</td>  \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>1</td>\n",
    "    <td>-4</td>  \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating $ATT$. To this end, we first need to estimate $Y_i(0)$ where $W_i=1$ (i.e., $i=1,2,3$). \n",
    "- For $i=1$, $X_1=3$ is the closest to $X_5=3$, so $\\hat Y_1(0)=Y_5(0)=9$;\n",
    "- For $i=2$, $X_2=1$ is the closest to $X_4=2$, so $\\hat Y_2(0)=Y_4(0)=0$;\n",
    "- For $i=3$, $X_3=10$ is the closest to $X_5=3$, so $\\hat Y_3(0)=Y_5(0)=9$.\n",
    "\n",
    "Therefore, the estimated average treatment effect on treated is\n",
    "$$\\widehat{ATT}=\\frac{1}{3}\\sum_{W_i=1}(Y_i(1)-\\hat Y_i(0))=\\frac{1}{3}\\left((6-9)+(1-0)+(0-9)\\right)=-3.7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Propensity Score Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighborhood-based matching, similar to $k$NN, suffers from the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and, therefore, computationally intractable in general. We tackle this challenge through the approach called **[Propensity Score Matching](https://en.wikipedia.org/wiki/Propensity_score_matching)**.\n",
    "\n",
    "Given the feature $X_i=x$, we define the **<font color=\"red\">propensity score</font>** of subject $i$ as the probability of receiving treatment under the treatment assignment mechanism $\\mathcal W$:\n",
    "\n",
    "<font color=\"red\">\n",
    "$$\\pi(x):=\\mathbb P[W_i=1|X_i=x]$$\n",
    "</font>\n",
    "An important property of the propensity score is that, for valid causal inference, it suffices to just condition on the propensity score $\\pi(X_i)$, instead of the original feature $X_i$. \n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"font-family:Comic Sans MS\">\n",
    "    <p style=\"color:red\">\n",
    "If the CIA and the common support assumption hold, we have\n",
    "        $$\\{Y_i(1),Y_i(0)\\}\\perp W_i|\\pi(X_i)=\\omega,\\mbox{ for all feature }\\omega\\in[0,1]. $$\n",
    "        </p>\n",
    "</span>    \n",
    "\n",
    "----\n",
    "The above property implies that, among the subjects with the same propensity score $\\pi(X_i)=\\omega$, the treatment assignment $W_i$ is identically and independently distributed between treated ($W_i=1$) and untreated ($W_i=0$).\n",
    "\n",
    "We are now ready to provide the procedure of Propensity Score Matching:\n",
    "\n",
    "---\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "- **Step 1.** Estimate the propensity score $\\hat{\\pi}(X_i)$ via logistic regression, $k$NN, classification tree, or other classification models.\n",
    "- **Step 2.** For each subject $i$ in the treatment group $W_i=1$, match subject $i$ with subject $j$ in the control group (i.e., $W_j=0$) which has the closest propensity score, i.e., $|\\hat\\pi(X_i)-\\hat\\pi(X_j)|$ is smallest for all $W_j=0$. We denote the estimated potential outcome of subject $i$ with $W_i=0$ as $\\hat Y_i(0)$.\n",
    "- **Step 3.** Conduct a balance check of the feature and the propensity score distribution between the treated and untreated in the matched sample.\n",
    "- **Step 4.** Estimate the ATT as:\n",
    "$$\\widehat{ATT}=\\frac{1}{n_1}\\sum_{W_i=1}(Y_i(1)-\\hat Y_i(0))$$\n",
    "We can also conduct $t-$test to examine whether $ATT$ is significantly away from 0. \n",
    "    \n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a PSM example in the following table (with $M=1$, so we only consider the nearest neighbor in matching):\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th>Subject </th>\n",
    "    <th>$W_i$</th> \n",
    "    <th>$Y_i(1)$</th>\n",
    "    <th>$Y_i(0)$</th>\n",
    "    <th>$\\hat\\pi(X_i)$</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>6</td>\n",
    "    <td>?</td>\n",
    "    <td>0.3</td>  \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "    <td>1</td>\n",
    "    <td>?</td>\n",
    "    <td>0.06</td>   \n",
    "    </tr>\n",
    "   <tr>\n",
    "<td>3</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>?</td>\n",
    "    <td>0.9</td>   \n",
    "     </tr>\n",
    "   <tr>\n",
    "    <td>4</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>0.2</td>  \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>5</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>9</td>\n",
    "    <td>0.3</td>  \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>1</td>\n",
    "    <td>0.05</td>  \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>0</td>\n",
    "    <td></td>\n",
    "    <td>1</td>\n",
    "    <td>0.03</td>  \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in evaluating $ATT$. To this end, we first need to estimate $Y_i(0)$ where $W_i=1$ (i.e., $i=1,2,3$). \n",
    "- For $i=1$, $\\hat\\pi(X_1)=0.3$ is the closest to $\\hat\\pi(X_5)=0.3$, so $\\hat Y_1(0)=Y_5(0)=9$;\n",
    "- For $i=2$, $\\hat\\pi(X_2)=0.1$ is the closest to $\\hat\\pi(X_4)=0.2$, so $\\hat Y_2(0)=Y_4(0)=0$;\n",
    "- For $i=3$, $\\hat\\pi(X_3)=0.9$ is the closest to $\\hat\\pi(X_5)=0.3$, so $\\hat Y_3(0)=Y_5(0)=9$.\n",
    "\n",
    "Therefore, the estimated average treatment effect on treated is\n",
    "$$\\widehat{ATT}=\\frac{1}{3}\\sum_{W_i=1}(Y_i(1)-\\hat Y_i(0))=\\frac{1}{3}\\left((6-9)+(1-0)+(0-9)\\right)=-3.7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now conduct the PSM analysis for the Didi's coupon data. First, we install the ``CausalInference`` package by entering ``pip install CausalInference`` in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first fit a logistic regression model to compute the propensity score of each subject in the data set.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "W = 'Coupon'\n",
    "X = ['Spending','ActiveDays']\n",
    "\n",
    "ps_model = LogisticRegression(C=1e6).fit(df_Didi_new[X], df_Didi_new[W])\n",
    "\n",
    "df_Didi_new['p_score'] = ps_model.predict_proba(df_Didi_new[X])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_active</th>\n",
       "      <th>new_spending</th>\n",
       "      <th>Coupon</th>\n",
       "      <th>Male</th>\n",
       "      <th>ActiveDays</th>\n",
       "      <th>Spending</th>\n",
       "      <th>p_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.316452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31.44</td>\n",
       "      <td>0.258008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>35.32</td>\n",
       "      <td>0.226393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.97</td>\n",
       "      <td>0.403553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.92</td>\n",
       "      <td>0.458536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.39</td>\n",
       "      <td>0.453354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>61.47</td>\n",
       "      <td>0.056596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.542475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36.74</td>\n",
       "      <td>0.215531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.31</td>\n",
       "      <td>0.389304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>48.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>73.84</td>\n",
       "      <td>0.050174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.70</td>\n",
       "      <td>0.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39.08</td>\n",
       "      <td>0.198467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>95.16</td>\n",
       "      <td>0.013244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.766041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.455227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.766041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.02</td>\n",
       "      <td>0.345770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93.66</td>\n",
       "      <td>0.014144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.766041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_active  new_spending  Coupon  Male  ActiveDays  Spending   p_score\n",
       "0           0           0.0       0     1           2     25.00  0.316452\n",
       "1           0           0.0       0     0           2     31.44  0.258008\n",
       "2           1          16.3       0     0           2     35.32  0.226393\n",
       "3           0           0.0       1     1           1     25.97  0.403553\n",
       "4           0           0.0       0     0           1     20.92  0.458536\n",
       "5           0           0.0       1     1           1     21.39  0.453354\n",
       "6           0           0.0       0     0           3     61.47  0.056596\n",
       "7           0           0.0       1     0           2      3.84  0.542475\n",
       "8           0           0.0       0     1           2     36.74  0.215531\n",
       "9           0           0.0       0     1           1     27.31  0.389304\n",
       "10          1          48.3       1     1           2     73.84  0.050174\n",
       "11          0           0.0       0     1           1     12.70  0.549609\n",
       "12          0           0.0       0     1           2     39.08  0.198467\n",
       "13          0           0.0       0     0           3     95.16  0.013244\n",
       "14          0           0.0       1     0           0      0.00  0.766041\n",
       "15          0           0.0       0     1           1     21.22  0.455227\n",
       "16          0           0.0       1     0           0      0.00  0.766041\n",
       "17          0           0.0       1     0           2     22.02  0.345770\n",
       "18          0           0.0       1     0           3     93.66  0.014144\n",
       "19          0           0.0       1     1           0      0.00  0.766041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Didi_new.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we conduct propensity score matching analysis to estimate the ATT.\n",
    "\n",
    "# First we change the maximum number of recursions in this notebook.\n",
    "import sys   \n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# Because matching is slow, we downsample the data set to 5000 subjects.\n",
    "\n",
    "df_Didi_downsample = df_Didi_new.sample(n=5000)\n",
    "\n",
    "cm = CausalModel(\n",
    "    Y=df_Didi_downsample[\"is_active\"].values, # Outcomes\n",
    "    D=df_Didi_downsample[\"Coupon\"].values,  # Treatment assignment\n",
    "    X=df_Didi_downsample[[\"p_score\"]].values # Features\n",
    ")\n",
    "\n",
    "cm.est_via_matching(matches=1,bias_adj=True)\n",
    "\n",
    "print(cm.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the PSM approach, though slow, produces similar estimates to the regression method for the causal effect of receiving a coupon on the activeness of a user.. We also emphasize that the standard error estimate using PSM is not accurate, so please just ignore it (see [this book](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html) for more on this issue). \n",
    "\n",
    "There are a couple of other ways to use propensity scores for causal inference other than matching. The first is to directly regress the outcome $Y_i$ on the treatment assignment $W_i$ and the estimated propensity score $\\hat{\\pi}(X_i)$, which we illustrate below using the Didi coupon case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS specification: is_active ~ beta_0 + beta_1 * Coupon + beta_2 * Male + beta_3 * ActiveDays + beta_4 * Spending + epsion\n",
    "\n",
    "\n",
    "features = ['Coupon','p_score']\n",
    "\n",
    "OLS_ps = sm.OLS(endog = df_Didi_new['is_active'], exog = sm.add_constant(df_Didi_new[features]))\n",
    "result_ps = OLS_ps.fit()\n",
    "print(result_ps.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we obtain a similar estimate to the estimation using OLS regression on all the features $X$.\n",
    "\n",
    "### Inverse Propensity Weighting Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another useful way of leveraging propensity scores is to (inversely) weight the outcome of each subject based on the propensity score. Specifically, under the CIA and the full support assumption, we estimate the average treatment effect based on the following identity:\n",
    "\n",
    "---------\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "$$ATE=\\mathbb E[Y_i(1)]-\\mathbb E[Y_i(0)]=\\mathbb E\\left[\\mathbb E\\left[\\frac{W_i Y_i(1)}{\\pi(X_i)}\\bigg|X_i\\right]\\right]-\\mathbb E\\left[\\mathbb E\\left[\\frac{(1-W_i)Y_i(0)}{1-\\pi(X_i)}\\bigg|X_i\\right]\\right]$$\n",
    "\n",
    "</font>    \n",
    "\n",
    "----------\n",
    "Under the CIA and the full support assumption, we can use the following [inverse propensity/probability weighting (IPW) estimator](http://www.rebeccabarter.com/blog/2017-07-05-ip-weighting/):\n",
    "\n",
    "-----\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "\n",
    "$$\\widehat{ATE}=\\frac{1}{n}\\sum_{W_i=1}\\frac{Y_i(1)}{\\hat\\pi(X_i)}-\\frac{1}{n}\\sum_{W_i=0}\\frac{Y_i(0)}{1-\\hat\\pi(X_i)},$$\n",
    "\n",
    "</font>    \n",
    "\n",
    "-------\n",
    "\n",
    "where $\\hat\\pi(X_i)$ is the estimated propensity score of the subject with feature $X_i$. We implement this idea with Didi's case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate ATE using inverse propensity weighting\n",
    "\n",
    "treat = df_Didi_new[df_Didi_new.Coupon==1]\n",
    "ctrl = df_Didi_new[df_Didi_new.Coupon==0]\n",
    "\n",
    "# Inverse Propensity Weighting\n",
    "treat['is_active_weighted'] = treat['is_active'] * 1/treat['p_score']\n",
    "ctrl['is_active_weighted'] = ctrl['is_active'] * 1/(1-ctrl['p_score'])\n",
    "\n",
    "# Compute the estimate\n",
    "IPW = (treat['is_active_weighted'].sum() - ctrl['is_active_weighted'].sum())/df_Didi_new.shape[0]\n",
    "\n",
    "print('The inverse propensity weighting estimate is %0.4f.' % IPW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the inverse propensity score weighting estimate is different from other estimates, this is because IPW usually has a high variance so that it may not be that accurate. We can use an adjusted IPW to correct such bias.\n",
    "\n",
    "As a final note, we emphasize that, without randomized experiments, both regression and matching are only compromised solutions to causal inference for observational studies. Typically, we are **unable** to obtain the unbiased estimation of the causal effects due to the selection bias issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
